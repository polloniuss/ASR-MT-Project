{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258e1547",
   "metadata": {},
   "source": [
    "# Wav2Vec 2.0 Compression (Shrinking Bigfoot paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5163567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing librairies\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import argparse\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import string\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import Wav2Vec2ProcessorWithLM, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import time\n",
    "start_time = time.time()\n",
    "from jiwer import wer, cer, mer, wip, wil\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "import soundfile\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "from torchaudio.models.wav2vec2.utils import import_huggingface_model\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "access_token = \"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\"\n",
    "!export HF_DATASETS_CACHE=\"/media/berenice/Healthy_Windows/Users/berenice/Documents/Storage/work/mission/wav2vec_model_storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfb0e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fbf23e03fc41ef9d60a250fc85c78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d337cefe5542fa84b88ffffc910193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e592041d00e144f9b597568ff1973323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/53.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset common_voice/uk to /media/berenice/Healthy_Windows/Users/berenice/Documents/Storage/work/mission/wav2vec_model_storage/mozilla-foundation___common_voice/uk/8.0.0/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset common_voice downloaded and prepared to /media/berenice/Healthy_Windows/Users/berenice/Documents/Storage/work/mission/wav2vec_model_storage/mozilla-foundation___common_voice/uk/8.0.0/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f4390fa2f54473bef338646cebe12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"uk\", use_auth_token=access_token, split=['train', 'test'], cache_dir=\"/media/berenice/Healthy_Windows/Users/berenice/Documents/Storage/work/mission/wav2vec_model_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d31798",
   "metadata": {},
   "source": [
    "# To-do list\n",
    "\n",
    "- create own yaml file (config)\n",
    "- download mozilla voices through load_dataset\n",
    "- copy code from (https://github.com/georgian-io/Knowledge-Distillation-Toolkit#demo)\n",
    "- update librispeech to huggingface dataset\n",
    "- bin model to pt model?\n",
    "- shrink size!\n",
    "\n",
    "# PyTorch Live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993e4094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = Wav2Vec2ForCTC.from_pretrained(\"Yehor/wav2vec2-xls-r-300m-uk-with-small-lm\", cache_dir=\"/media/berenice/Healthy_Windows/Users/berenice/Documents/Storage/work/mission/wav2vec_model_storage\")\n",
    "model = import_huggingface_model(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0671f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing weight_norm from ConvolutionalPositionalEmbedding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully exported\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "scripted_model = torch.jit.script(model)\n",
    "\"\"\"\n",
    "optimized_model = optimize_for_mobile(scripted_model)\n",
    "spec = Path(\"live.spec.json\").read_text()\n",
    "\n",
    "extra_files = {}\n",
    "extra_files[\"model/live.spec.json\"] = spec\n",
    "optimized_model._save_for_lite_interpreter(\"model_live.ptl\", _extra_files=extra_files)\n",
    "\"\"\"\n",
    "print(\"model successfully exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cef6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the floating point version of this module:\n",
      "Wav2Vec2Model(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "      )\n",
      "      (1): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (2): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (3): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (4): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (5): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "      )\n",
      "      (6): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (feature_projection): FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
      "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (readout): Linear(in_features=1024, out_features=40, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "and now the quantized version:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Model(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "      )\n",
      "      (1): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (2): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (3): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (4): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "      )\n",
      "      (5): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "      )\n",
      "      (6): ConvLayerBlock(\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (feature_projection): FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
      "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): EncoderLayer(\n",
      "          (attention): SelfAttention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (readout): DynamicQuantizedLinear(in_features=1024, out_features=40, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# this is the call that does the work\n",
    "model.eval()\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "# nn.LSTM, nn.Linear\n",
    "# show the changes that were made\n",
    "print('Here is the floating point version of this module:')\n",
    "print(model)\n",
    "print('')\n",
    "print('and now the quantized version:')\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc6fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 1262078.197\n",
      "model:  int8  \t Size (KB): 354529.837\n",
      "3.56 times smaller\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "# compare the sizes\n",
    "f=print_size_of_model(model,\"fp32\")\n",
    "q=print_size_of_model(quantized_model,\"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66f08b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(quantized_model.state_dict(), 'wav2vec_small_lm_live.pt')\n",
    "\n",
    "torch.save({'model01': quantized_model.state_dict(), 'wav2vec_small_lm_live2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a5a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchaudio.models.wav2vec2.model.Wav2Vec2Model'>\n"
     ]
    }
   ],
   "source": [
    "print(type(quantized_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa94cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': quantized_model.state_dict()}, \"wav2vec_small_lm_live2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25fd1928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['encoder.feature_projection.projection.weight', 'encoder.feature_projection.projection.bias', 'encoder.transformer.layers.0.attention.k_proj.weight', 'encoder.transformer.layers.0.attention.k_proj.bias', 'encoder.transformer.layers.0.attention.v_proj.weight', 'encoder.transformer.layers.0.attention.v_proj.bias', 'encoder.transformer.layers.0.attention.q_proj.weight', 'encoder.transformer.layers.0.attention.q_proj.bias', 'encoder.transformer.layers.0.attention.out_proj.weight', 'encoder.transformer.layers.0.attention.out_proj.bias', 'encoder.transformer.layers.0.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.0.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.0.feed_forward.output_dense.weight', 'encoder.transformer.layers.0.feed_forward.output_dense.bias', 'encoder.transformer.layers.1.attention.k_proj.weight', 'encoder.transformer.layers.1.attention.k_proj.bias', 'encoder.transformer.layers.1.attention.v_proj.weight', 'encoder.transformer.layers.1.attention.v_proj.bias', 'encoder.transformer.layers.1.attention.q_proj.weight', 'encoder.transformer.layers.1.attention.q_proj.bias', 'encoder.transformer.layers.1.attention.out_proj.weight', 'encoder.transformer.layers.1.attention.out_proj.bias', 'encoder.transformer.layers.1.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.1.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.1.feed_forward.output_dense.weight', 'encoder.transformer.layers.1.feed_forward.output_dense.bias', 'encoder.transformer.layers.2.attention.k_proj.weight', 'encoder.transformer.layers.2.attention.k_proj.bias', 'encoder.transformer.layers.2.attention.v_proj.weight', 'encoder.transformer.layers.2.attention.v_proj.bias', 'encoder.transformer.layers.2.attention.q_proj.weight', 'encoder.transformer.layers.2.attention.q_proj.bias', 'encoder.transformer.layers.2.attention.out_proj.weight', 'encoder.transformer.layers.2.attention.out_proj.bias', 'encoder.transformer.layers.2.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.2.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.2.feed_forward.output_dense.weight', 'encoder.transformer.layers.2.feed_forward.output_dense.bias', 'encoder.transformer.layers.3.attention.k_proj.weight', 'encoder.transformer.layers.3.attention.k_proj.bias', 'encoder.transformer.layers.3.attention.v_proj.weight', 'encoder.transformer.layers.3.attention.v_proj.bias', 'encoder.transformer.layers.3.attention.q_proj.weight', 'encoder.transformer.layers.3.attention.q_proj.bias', 'encoder.transformer.layers.3.attention.out_proj.weight', 'encoder.transformer.layers.3.attention.out_proj.bias', 'encoder.transformer.layers.3.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.3.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.3.feed_forward.output_dense.weight', 'encoder.transformer.layers.3.feed_forward.output_dense.bias', 'encoder.transformer.layers.4.attention.k_proj.weight', 'encoder.transformer.layers.4.attention.k_proj.bias', 'encoder.transformer.layers.4.attention.v_proj.weight', 'encoder.transformer.layers.4.attention.v_proj.bias', 'encoder.transformer.layers.4.attention.q_proj.weight', 'encoder.transformer.layers.4.attention.q_proj.bias', 'encoder.transformer.layers.4.attention.out_proj.weight', 'encoder.transformer.layers.4.attention.out_proj.bias', 'encoder.transformer.layers.4.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.4.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.4.feed_forward.output_dense.weight', 'encoder.transformer.layers.4.feed_forward.output_dense.bias', 'encoder.transformer.layers.5.attention.k_proj.weight', 'encoder.transformer.layers.5.attention.k_proj.bias', 'encoder.transformer.layers.5.attention.v_proj.weight', 'encoder.transformer.layers.5.attention.v_proj.bias', 'encoder.transformer.layers.5.attention.q_proj.weight', 'encoder.transformer.layers.5.attention.q_proj.bias', 'encoder.transformer.layers.5.attention.out_proj.weight', 'encoder.transformer.layers.5.attention.out_proj.bias', 'encoder.transformer.layers.5.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.5.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.5.feed_forward.output_dense.weight', 'encoder.transformer.layers.5.feed_forward.output_dense.bias', 'encoder.transformer.layers.6.attention.k_proj.weight', 'encoder.transformer.layers.6.attention.k_proj.bias', 'encoder.transformer.layers.6.attention.v_proj.weight', 'encoder.transformer.layers.6.attention.v_proj.bias', 'encoder.transformer.layers.6.attention.q_proj.weight', 'encoder.transformer.layers.6.attention.q_proj.bias', 'encoder.transformer.layers.6.attention.out_proj.weight', 'encoder.transformer.layers.6.attention.out_proj.bias', 'encoder.transformer.layers.6.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.6.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.6.feed_forward.output_dense.weight', 'encoder.transformer.layers.6.feed_forward.output_dense.bias', 'encoder.transformer.layers.7.attention.k_proj.weight', 'encoder.transformer.layers.7.attention.k_proj.bias', 'encoder.transformer.layers.7.attention.v_proj.weight', 'encoder.transformer.layers.7.attention.v_proj.bias', 'encoder.transformer.layers.7.attention.q_proj.weight', 'encoder.transformer.layers.7.attention.q_proj.bias', 'encoder.transformer.layers.7.attention.out_proj.weight', 'encoder.transformer.layers.7.attention.out_proj.bias', 'encoder.transformer.layers.7.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.7.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.7.feed_forward.output_dense.weight', 'encoder.transformer.layers.7.feed_forward.output_dense.bias', 'encoder.transformer.layers.8.attention.k_proj.weight', 'encoder.transformer.layers.8.attention.k_proj.bias', 'encoder.transformer.layers.8.attention.v_proj.weight', 'encoder.transformer.layers.8.attention.v_proj.bias', 'encoder.transformer.layers.8.attention.q_proj.weight', 'encoder.transformer.layers.8.attention.q_proj.bias', 'encoder.transformer.layers.8.attention.out_proj.weight', 'encoder.transformer.layers.8.attention.out_proj.bias', 'encoder.transformer.layers.8.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.8.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.8.feed_forward.output_dense.weight', 'encoder.transformer.layers.8.feed_forward.output_dense.bias', 'encoder.transformer.layers.9.attention.k_proj.weight', 'encoder.transformer.layers.9.attention.k_proj.bias', 'encoder.transformer.layers.9.attention.v_proj.weight', 'encoder.transformer.layers.9.attention.v_proj.bias', 'encoder.transformer.layers.9.attention.q_proj.weight', 'encoder.transformer.layers.9.attention.q_proj.bias', 'encoder.transformer.layers.9.attention.out_proj.weight', 'encoder.transformer.layers.9.attention.out_proj.bias', 'encoder.transformer.layers.9.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.9.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.9.feed_forward.output_dense.weight', 'encoder.transformer.layers.9.feed_forward.output_dense.bias', 'encoder.transformer.layers.10.attention.k_proj.weight', 'encoder.transformer.layers.10.attention.k_proj.bias', 'encoder.transformer.layers.10.attention.v_proj.weight', 'encoder.transformer.layers.10.attention.v_proj.bias', 'encoder.transformer.layers.10.attention.q_proj.weight', 'encoder.transformer.layers.10.attention.q_proj.bias', 'encoder.transformer.layers.10.attention.out_proj.weight', 'encoder.transformer.layers.10.attention.out_proj.bias', 'encoder.transformer.layers.10.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.10.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.10.feed_forward.output_dense.weight', 'encoder.transformer.layers.10.feed_forward.output_dense.bias', 'encoder.transformer.layers.11.attention.k_proj.weight', 'encoder.transformer.layers.11.attention.k_proj.bias', 'encoder.transformer.layers.11.attention.v_proj.weight', 'encoder.transformer.layers.11.attention.v_proj.bias', 'encoder.transformer.layers.11.attention.q_proj.weight', 'encoder.transformer.layers.11.attention.q_proj.bias', 'encoder.transformer.layers.11.attention.out_proj.weight', 'encoder.transformer.layers.11.attention.out_proj.bias', 'encoder.transformer.layers.11.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.11.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.11.feed_forward.output_dense.weight', 'encoder.transformer.layers.11.feed_forward.output_dense.bias', 'encoder.transformer.layers.12.attention.k_proj.weight', 'encoder.transformer.layers.12.attention.k_proj.bias', 'encoder.transformer.layers.12.attention.v_proj.weight', 'encoder.transformer.layers.12.attention.v_proj.bias', 'encoder.transformer.layers.12.attention.q_proj.weight', 'encoder.transformer.layers.12.attention.q_proj.bias', 'encoder.transformer.layers.12.attention.out_proj.weight', 'encoder.transformer.layers.12.attention.out_proj.bias', 'encoder.transformer.layers.12.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.12.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.12.feed_forward.output_dense.weight', 'encoder.transformer.layers.12.feed_forward.output_dense.bias', 'encoder.transformer.layers.13.attention.k_proj.weight', 'encoder.transformer.layers.13.attention.k_proj.bias', 'encoder.transformer.layers.13.attention.v_proj.weight', 'encoder.transformer.layers.13.attention.v_proj.bias', 'encoder.transformer.layers.13.attention.q_proj.weight', 'encoder.transformer.layers.13.attention.q_proj.bias', 'encoder.transformer.layers.13.attention.out_proj.weight', 'encoder.transformer.layers.13.attention.out_proj.bias', 'encoder.transformer.layers.13.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.13.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.13.feed_forward.output_dense.weight', 'encoder.transformer.layers.13.feed_forward.output_dense.bias', 'encoder.transformer.layers.14.attention.k_proj.weight', 'encoder.transformer.layers.14.attention.k_proj.bias', 'encoder.transformer.layers.14.attention.v_proj.weight', 'encoder.transformer.layers.14.attention.v_proj.bias', 'encoder.transformer.layers.14.attention.q_proj.weight', 'encoder.transformer.layers.14.attention.q_proj.bias', 'encoder.transformer.layers.14.attention.out_proj.weight', 'encoder.transformer.layers.14.attention.out_proj.bias', 'encoder.transformer.layers.14.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.14.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.14.feed_forward.output_dense.weight', 'encoder.transformer.layers.14.feed_forward.output_dense.bias', 'encoder.transformer.layers.15.attention.k_proj.weight', 'encoder.transformer.layers.15.attention.k_proj.bias', 'encoder.transformer.layers.15.attention.v_proj.weight', 'encoder.transformer.layers.15.attention.v_proj.bias', 'encoder.transformer.layers.15.attention.q_proj.weight', 'encoder.transformer.layers.15.attention.q_proj.bias', 'encoder.transformer.layers.15.attention.out_proj.weight', 'encoder.transformer.layers.15.attention.out_proj.bias', 'encoder.transformer.layers.15.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.15.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.15.feed_forward.output_dense.weight', 'encoder.transformer.layers.15.feed_forward.output_dense.bias', 'encoder.transformer.layers.16.attention.k_proj.weight', 'encoder.transformer.layers.16.attention.k_proj.bias', 'encoder.transformer.layers.16.attention.v_proj.weight', 'encoder.transformer.layers.16.attention.v_proj.bias', 'encoder.transformer.layers.16.attention.q_proj.weight', 'encoder.transformer.layers.16.attention.q_proj.bias', 'encoder.transformer.layers.16.attention.out_proj.weight', 'encoder.transformer.layers.16.attention.out_proj.bias', 'encoder.transformer.layers.16.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.16.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.16.feed_forward.output_dense.weight', 'encoder.transformer.layers.16.feed_forward.output_dense.bias', 'encoder.transformer.layers.17.attention.k_proj.weight', 'encoder.transformer.layers.17.attention.k_proj.bias', 'encoder.transformer.layers.17.attention.v_proj.weight', 'encoder.transformer.layers.17.attention.v_proj.bias', 'encoder.transformer.layers.17.attention.q_proj.weight', 'encoder.transformer.layers.17.attention.q_proj.bias', 'encoder.transformer.layers.17.attention.out_proj.weight', 'encoder.transformer.layers.17.attention.out_proj.bias', 'encoder.transformer.layers.17.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.17.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.17.feed_forward.output_dense.weight', 'encoder.transformer.layers.17.feed_forward.output_dense.bias', 'encoder.transformer.layers.18.attention.k_proj.weight', 'encoder.transformer.layers.18.attention.k_proj.bias', 'encoder.transformer.layers.18.attention.v_proj.weight', 'encoder.transformer.layers.18.attention.v_proj.bias', 'encoder.transformer.layers.18.attention.q_proj.weight', 'encoder.transformer.layers.18.attention.q_proj.bias', 'encoder.transformer.layers.18.attention.out_proj.weight', 'encoder.transformer.layers.18.attention.out_proj.bias', 'encoder.transformer.layers.18.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.18.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.18.feed_forward.output_dense.weight', 'encoder.transformer.layers.18.feed_forward.output_dense.bias', 'encoder.transformer.layers.19.attention.k_proj.weight', 'encoder.transformer.layers.19.attention.k_proj.bias', 'encoder.transformer.layers.19.attention.v_proj.weight', 'encoder.transformer.layers.19.attention.v_proj.bias', 'encoder.transformer.layers.19.attention.q_proj.weight', 'encoder.transformer.layers.19.attention.q_proj.bias', 'encoder.transformer.layers.19.attention.out_proj.weight', 'encoder.transformer.layers.19.attention.out_proj.bias', 'encoder.transformer.layers.19.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.19.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.19.feed_forward.output_dense.weight', 'encoder.transformer.layers.19.feed_forward.output_dense.bias', 'encoder.transformer.layers.20.attention.k_proj.weight', 'encoder.transformer.layers.20.attention.k_proj.bias', 'encoder.transformer.layers.20.attention.v_proj.weight', 'encoder.transformer.layers.20.attention.v_proj.bias', 'encoder.transformer.layers.20.attention.q_proj.weight', 'encoder.transformer.layers.20.attention.q_proj.bias', 'encoder.transformer.layers.20.attention.out_proj.weight', 'encoder.transformer.layers.20.attention.out_proj.bias', 'encoder.transformer.layers.20.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.20.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.20.feed_forward.output_dense.weight', 'encoder.transformer.layers.20.feed_forward.output_dense.bias', 'encoder.transformer.layers.21.attention.k_proj.weight', 'encoder.transformer.layers.21.attention.k_proj.bias', 'encoder.transformer.layers.21.attention.v_proj.weight', 'encoder.transformer.layers.21.attention.v_proj.bias', 'encoder.transformer.layers.21.attention.q_proj.weight', 'encoder.transformer.layers.21.attention.q_proj.bias', 'encoder.transformer.layers.21.attention.out_proj.weight', 'encoder.transformer.layers.21.attention.out_proj.bias', 'encoder.transformer.layers.21.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.21.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.21.feed_forward.output_dense.weight', 'encoder.transformer.layers.21.feed_forward.output_dense.bias', 'encoder.transformer.layers.22.attention.k_proj.weight', 'encoder.transformer.layers.22.attention.k_proj.bias', 'encoder.transformer.layers.22.attention.v_proj.weight', 'encoder.transformer.layers.22.attention.v_proj.bias', 'encoder.transformer.layers.22.attention.q_proj.weight', 'encoder.transformer.layers.22.attention.q_proj.bias', 'encoder.transformer.layers.22.attention.out_proj.weight', 'encoder.transformer.layers.22.attention.out_proj.bias', 'encoder.transformer.layers.22.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.22.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.22.feed_forward.output_dense.weight', 'encoder.transformer.layers.22.feed_forward.output_dense.bias', 'encoder.transformer.layers.23.attention.k_proj.weight', 'encoder.transformer.layers.23.attention.k_proj.bias', 'encoder.transformer.layers.23.attention.v_proj.weight', 'encoder.transformer.layers.23.attention.v_proj.bias', 'encoder.transformer.layers.23.attention.q_proj.weight', 'encoder.transformer.layers.23.attention.q_proj.bias', 'encoder.transformer.layers.23.attention.out_proj.weight', 'encoder.transformer.layers.23.attention.out_proj.bias', 'encoder.transformer.layers.23.feed_forward.intermediate_dense.weight', 'encoder.transformer.layers.23.feed_forward.intermediate_dense.bias', 'encoder.transformer.layers.23.feed_forward.output_dense.weight', 'encoder.transformer.layers.23.feed_forward.output_dense.bias', 'encoder.readout.weight', 'encoder.readout.bias'], unexpected_keys=['encoder.feature_projection.projection.scale', 'encoder.feature_projection.projection.zero_point', 'encoder.feature_projection.projection._packed_params.dtype', 'encoder.feature_projection.projection._packed_params._packed_params', 'encoder.transformer.layers.0.attention.k_proj.scale', 'encoder.transformer.layers.0.attention.k_proj.zero_point', 'encoder.transformer.layers.0.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.0.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.0.attention.v_proj.scale', 'encoder.transformer.layers.0.attention.v_proj.zero_point', 'encoder.transformer.layers.0.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.0.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.0.attention.q_proj.scale', 'encoder.transformer.layers.0.attention.q_proj.zero_point', 'encoder.transformer.layers.0.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.0.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.0.attention.out_proj.scale', 'encoder.transformer.layers.0.attention.out_proj.zero_point', 'encoder.transformer.layers.0.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.0.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.0.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.0.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.0.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.0.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.0.feed_forward.output_dense.scale', 'encoder.transformer.layers.0.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.0.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.0.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.1.attention.k_proj.scale', 'encoder.transformer.layers.1.attention.k_proj.zero_point', 'encoder.transformer.layers.1.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.1.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.1.attention.v_proj.scale', 'encoder.transformer.layers.1.attention.v_proj.zero_point', 'encoder.transformer.layers.1.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.1.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.1.attention.q_proj.scale', 'encoder.transformer.layers.1.attention.q_proj.zero_point', 'encoder.transformer.layers.1.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.1.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.1.attention.out_proj.scale', 'encoder.transformer.layers.1.attention.out_proj.zero_point', 'encoder.transformer.layers.1.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.1.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.1.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.1.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.1.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.1.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.1.feed_forward.output_dense.scale', 'encoder.transformer.layers.1.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.1.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.1.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.2.attention.k_proj.scale', 'encoder.transformer.layers.2.attention.k_proj.zero_point', 'encoder.transformer.layers.2.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.2.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.2.attention.v_proj.scale', 'encoder.transformer.layers.2.attention.v_proj.zero_point', 'encoder.transformer.layers.2.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.2.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.2.attention.q_proj.scale', 'encoder.transformer.layers.2.attention.q_proj.zero_point', 'encoder.transformer.layers.2.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.2.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.2.attention.out_proj.scale', 'encoder.transformer.layers.2.attention.out_proj.zero_point', 'encoder.transformer.layers.2.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.2.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.2.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.2.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.2.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.2.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.2.feed_forward.output_dense.scale', 'encoder.transformer.layers.2.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.2.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.2.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.3.attention.k_proj.scale', 'encoder.transformer.layers.3.attention.k_proj.zero_point', 'encoder.transformer.layers.3.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.3.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.3.attention.v_proj.scale', 'encoder.transformer.layers.3.attention.v_proj.zero_point', 'encoder.transformer.layers.3.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.3.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.3.attention.q_proj.scale', 'encoder.transformer.layers.3.attention.q_proj.zero_point', 'encoder.transformer.layers.3.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.3.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.3.attention.out_proj.scale', 'encoder.transformer.layers.3.attention.out_proj.zero_point', 'encoder.transformer.layers.3.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.3.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.3.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.3.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.3.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.3.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.3.feed_forward.output_dense.scale', 'encoder.transformer.layers.3.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.3.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.3.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.4.attention.k_proj.scale', 'encoder.transformer.layers.4.attention.k_proj.zero_point', 'encoder.transformer.layers.4.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.4.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.4.attention.v_proj.scale', 'encoder.transformer.layers.4.attention.v_proj.zero_point', 'encoder.transformer.layers.4.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.4.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.4.attention.q_proj.scale', 'encoder.transformer.layers.4.attention.q_proj.zero_point', 'encoder.transformer.layers.4.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.4.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.4.attention.out_proj.scale', 'encoder.transformer.layers.4.attention.out_proj.zero_point', 'encoder.transformer.layers.4.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.4.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.4.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.4.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.4.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.4.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.4.feed_forward.output_dense.scale', 'encoder.transformer.layers.4.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.4.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.4.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.5.attention.k_proj.scale', 'encoder.transformer.layers.5.attention.k_proj.zero_point', 'encoder.transformer.layers.5.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.5.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.5.attention.v_proj.scale', 'encoder.transformer.layers.5.attention.v_proj.zero_point', 'encoder.transformer.layers.5.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.5.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.5.attention.q_proj.scale', 'encoder.transformer.layers.5.attention.q_proj.zero_point', 'encoder.transformer.layers.5.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.5.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.5.attention.out_proj.scale', 'encoder.transformer.layers.5.attention.out_proj.zero_point', 'encoder.transformer.layers.5.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.5.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.5.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.5.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.5.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.5.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.5.feed_forward.output_dense.scale', 'encoder.transformer.layers.5.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.5.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.5.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.6.attention.k_proj.scale', 'encoder.transformer.layers.6.attention.k_proj.zero_point', 'encoder.transformer.layers.6.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.6.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.6.attention.v_proj.scale', 'encoder.transformer.layers.6.attention.v_proj.zero_point', 'encoder.transformer.layers.6.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.6.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.6.attention.q_proj.scale', 'encoder.transformer.layers.6.attention.q_proj.zero_point', 'encoder.transformer.layers.6.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.6.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.6.attention.out_proj.scale', 'encoder.transformer.layers.6.attention.out_proj.zero_point', 'encoder.transformer.layers.6.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.6.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.6.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.6.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.6.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.6.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.6.feed_forward.output_dense.scale', 'encoder.transformer.layers.6.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.6.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.6.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.7.attention.k_proj.scale', 'encoder.transformer.layers.7.attention.k_proj.zero_point', 'encoder.transformer.layers.7.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.7.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.7.attention.v_proj.scale', 'encoder.transformer.layers.7.attention.v_proj.zero_point', 'encoder.transformer.layers.7.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.7.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.7.attention.q_proj.scale', 'encoder.transformer.layers.7.attention.q_proj.zero_point', 'encoder.transformer.layers.7.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.7.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.7.attention.out_proj.scale', 'encoder.transformer.layers.7.attention.out_proj.zero_point', 'encoder.transformer.layers.7.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.7.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.7.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.7.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.7.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.7.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.7.feed_forward.output_dense.scale', 'encoder.transformer.layers.7.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.7.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.7.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.8.attention.k_proj.scale', 'encoder.transformer.layers.8.attention.k_proj.zero_point', 'encoder.transformer.layers.8.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.8.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.8.attention.v_proj.scale', 'encoder.transformer.layers.8.attention.v_proj.zero_point', 'encoder.transformer.layers.8.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.8.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.8.attention.q_proj.scale', 'encoder.transformer.layers.8.attention.q_proj.zero_point', 'encoder.transformer.layers.8.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.8.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.8.attention.out_proj.scale', 'encoder.transformer.layers.8.attention.out_proj.zero_point', 'encoder.transformer.layers.8.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.8.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.8.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.8.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.8.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.8.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.8.feed_forward.output_dense.scale', 'encoder.transformer.layers.8.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.8.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.8.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.9.attention.k_proj.scale', 'encoder.transformer.layers.9.attention.k_proj.zero_point', 'encoder.transformer.layers.9.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.9.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.9.attention.v_proj.scale', 'encoder.transformer.layers.9.attention.v_proj.zero_point', 'encoder.transformer.layers.9.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.9.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.9.attention.q_proj.scale', 'encoder.transformer.layers.9.attention.q_proj.zero_point', 'encoder.transformer.layers.9.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.9.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.9.attention.out_proj.scale', 'encoder.transformer.layers.9.attention.out_proj.zero_point', 'encoder.transformer.layers.9.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.9.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.9.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.9.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.9.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.9.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.9.feed_forward.output_dense.scale', 'encoder.transformer.layers.9.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.9.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.9.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.10.attention.k_proj.scale', 'encoder.transformer.layers.10.attention.k_proj.zero_point', 'encoder.transformer.layers.10.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.10.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.10.attention.v_proj.scale', 'encoder.transformer.layers.10.attention.v_proj.zero_point', 'encoder.transformer.layers.10.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.10.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.10.attention.q_proj.scale', 'encoder.transformer.layers.10.attention.q_proj.zero_point', 'encoder.transformer.layers.10.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.10.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.10.attention.out_proj.scale', 'encoder.transformer.layers.10.attention.out_proj.zero_point', 'encoder.transformer.layers.10.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.10.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.10.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.10.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.10.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.10.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.10.feed_forward.output_dense.scale', 'encoder.transformer.layers.10.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.10.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.10.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.11.attention.k_proj.scale', 'encoder.transformer.layers.11.attention.k_proj.zero_point', 'encoder.transformer.layers.11.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.11.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.11.attention.v_proj.scale', 'encoder.transformer.layers.11.attention.v_proj.zero_point', 'encoder.transformer.layers.11.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.11.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.11.attention.q_proj.scale', 'encoder.transformer.layers.11.attention.q_proj.zero_point', 'encoder.transformer.layers.11.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.11.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.11.attention.out_proj.scale', 'encoder.transformer.layers.11.attention.out_proj.zero_point', 'encoder.transformer.layers.11.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.11.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.11.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.11.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.11.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.11.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.11.feed_forward.output_dense.scale', 'encoder.transformer.layers.11.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.11.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.11.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.12.attention.k_proj.scale', 'encoder.transformer.layers.12.attention.k_proj.zero_point', 'encoder.transformer.layers.12.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.12.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.12.attention.v_proj.scale', 'encoder.transformer.layers.12.attention.v_proj.zero_point', 'encoder.transformer.layers.12.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.12.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.12.attention.q_proj.scale', 'encoder.transformer.layers.12.attention.q_proj.zero_point', 'encoder.transformer.layers.12.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.12.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.12.attention.out_proj.scale', 'encoder.transformer.layers.12.attention.out_proj.zero_point', 'encoder.transformer.layers.12.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.12.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.12.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.12.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.12.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.12.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.12.feed_forward.output_dense.scale', 'encoder.transformer.layers.12.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.12.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.12.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.13.attention.k_proj.scale', 'encoder.transformer.layers.13.attention.k_proj.zero_point', 'encoder.transformer.layers.13.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.13.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.13.attention.v_proj.scale', 'encoder.transformer.layers.13.attention.v_proj.zero_point', 'encoder.transformer.layers.13.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.13.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.13.attention.q_proj.scale', 'encoder.transformer.layers.13.attention.q_proj.zero_point', 'encoder.transformer.layers.13.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.13.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.13.attention.out_proj.scale', 'encoder.transformer.layers.13.attention.out_proj.zero_point', 'encoder.transformer.layers.13.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.13.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.13.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.13.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.13.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.13.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.13.feed_forward.output_dense.scale', 'encoder.transformer.layers.13.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.13.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.13.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.14.attention.k_proj.scale', 'encoder.transformer.layers.14.attention.k_proj.zero_point', 'encoder.transformer.layers.14.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.14.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.14.attention.v_proj.scale', 'encoder.transformer.layers.14.attention.v_proj.zero_point', 'encoder.transformer.layers.14.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.14.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.14.attention.q_proj.scale', 'encoder.transformer.layers.14.attention.q_proj.zero_point', 'encoder.transformer.layers.14.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.14.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.14.attention.out_proj.scale', 'encoder.transformer.layers.14.attention.out_proj.zero_point', 'encoder.transformer.layers.14.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.14.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.14.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.14.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.14.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.14.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.14.feed_forward.output_dense.scale', 'encoder.transformer.layers.14.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.14.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.14.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.15.attention.k_proj.scale', 'encoder.transformer.layers.15.attention.k_proj.zero_point', 'encoder.transformer.layers.15.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.15.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.15.attention.v_proj.scale', 'encoder.transformer.layers.15.attention.v_proj.zero_point', 'encoder.transformer.layers.15.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.15.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.15.attention.q_proj.scale', 'encoder.transformer.layers.15.attention.q_proj.zero_point', 'encoder.transformer.layers.15.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.15.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.15.attention.out_proj.scale', 'encoder.transformer.layers.15.attention.out_proj.zero_point', 'encoder.transformer.layers.15.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.15.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.15.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.15.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.15.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.15.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.15.feed_forward.output_dense.scale', 'encoder.transformer.layers.15.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.15.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.15.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.16.attention.k_proj.scale', 'encoder.transformer.layers.16.attention.k_proj.zero_point', 'encoder.transformer.layers.16.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.16.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.16.attention.v_proj.scale', 'encoder.transformer.layers.16.attention.v_proj.zero_point', 'encoder.transformer.layers.16.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.16.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.16.attention.q_proj.scale', 'encoder.transformer.layers.16.attention.q_proj.zero_point', 'encoder.transformer.layers.16.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.16.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.16.attention.out_proj.scale', 'encoder.transformer.layers.16.attention.out_proj.zero_point', 'encoder.transformer.layers.16.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.16.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.16.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.16.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.16.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.16.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.16.feed_forward.output_dense.scale', 'encoder.transformer.layers.16.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.16.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.16.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.17.attention.k_proj.scale', 'encoder.transformer.layers.17.attention.k_proj.zero_point', 'encoder.transformer.layers.17.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.17.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.17.attention.v_proj.scale', 'encoder.transformer.layers.17.attention.v_proj.zero_point', 'encoder.transformer.layers.17.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.17.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.17.attention.q_proj.scale', 'encoder.transformer.layers.17.attention.q_proj.zero_point', 'encoder.transformer.layers.17.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.17.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.17.attention.out_proj.scale', 'encoder.transformer.layers.17.attention.out_proj.zero_point', 'encoder.transformer.layers.17.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.17.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.17.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.17.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.17.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.17.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.17.feed_forward.output_dense.scale', 'encoder.transformer.layers.17.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.17.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.17.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.18.attention.k_proj.scale', 'encoder.transformer.layers.18.attention.k_proj.zero_point', 'encoder.transformer.layers.18.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.18.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.18.attention.v_proj.scale', 'encoder.transformer.layers.18.attention.v_proj.zero_point', 'encoder.transformer.layers.18.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.18.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.18.attention.q_proj.scale', 'encoder.transformer.layers.18.attention.q_proj.zero_point', 'encoder.transformer.layers.18.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.18.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.18.attention.out_proj.scale', 'encoder.transformer.layers.18.attention.out_proj.zero_point', 'encoder.transformer.layers.18.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.18.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.18.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.18.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.18.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.18.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.18.feed_forward.output_dense.scale', 'encoder.transformer.layers.18.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.18.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.18.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.19.attention.k_proj.scale', 'encoder.transformer.layers.19.attention.k_proj.zero_point', 'encoder.transformer.layers.19.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.19.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.19.attention.v_proj.scale', 'encoder.transformer.layers.19.attention.v_proj.zero_point', 'encoder.transformer.layers.19.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.19.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.19.attention.q_proj.scale', 'encoder.transformer.layers.19.attention.q_proj.zero_point', 'encoder.transformer.layers.19.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.19.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.19.attention.out_proj.scale', 'encoder.transformer.layers.19.attention.out_proj.zero_point', 'encoder.transformer.layers.19.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.19.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.19.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.19.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.19.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.19.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.19.feed_forward.output_dense.scale', 'encoder.transformer.layers.19.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.19.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.19.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.20.attention.k_proj.scale', 'encoder.transformer.layers.20.attention.k_proj.zero_point', 'encoder.transformer.layers.20.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.20.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.20.attention.v_proj.scale', 'encoder.transformer.layers.20.attention.v_proj.zero_point', 'encoder.transformer.layers.20.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.20.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.20.attention.q_proj.scale', 'encoder.transformer.layers.20.attention.q_proj.zero_point', 'encoder.transformer.layers.20.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.20.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.20.attention.out_proj.scale', 'encoder.transformer.layers.20.attention.out_proj.zero_point', 'encoder.transformer.layers.20.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.20.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.20.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.20.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.20.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.20.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.20.feed_forward.output_dense.scale', 'encoder.transformer.layers.20.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.20.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.20.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.21.attention.k_proj.scale', 'encoder.transformer.layers.21.attention.k_proj.zero_point', 'encoder.transformer.layers.21.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.21.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.21.attention.v_proj.scale', 'encoder.transformer.layers.21.attention.v_proj.zero_point', 'encoder.transformer.layers.21.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.21.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.21.attention.q_proj.scale', 'encoder.transformer.layers.21.attention.q_proj.zero_point', 'encoder.transformer.layers.21.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.21.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.21.attention.out_proj.scale', 'encoder.transformer.layers.21.attention.out_proj.zero_point', 'encoder.transformer.layers.21.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.21.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.21.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.21.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.21.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.21.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.21.feed_forward.output_dense.scale', 'encoder.transformer.layers.21.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.21.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.21.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.22.attention.k_proj.scale', 'encoder.transformer.layers.22.attention.k_proj.zero_point', 'encoder.transformer.layers.22.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.22.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.22.attention.v_proj.scale', 'encoder.transformer.layers.22.attention.v_proj.zero_point', 'encoder.transformer.layers.22.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.22.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.22.attention.q_proj.scale', 'encoder.transformer.layers.22.attention.q_proj.zero_point', 'encoder.transformer.layers.22.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.22.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.22.attention.out_proj.scale', 'encoder.transformer.layers.22.attention.out_proj.zero_point', 'encoder.transformer.layers.22.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.22.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.22.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.22.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.22.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.22.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.22.feed_forward.output_dense.scale', 'encoder.transformer.layers.22.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.22.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.22.feed_forward.output_dense._packed_params._packed_params', 'encoder.transformer.layers.23.attention.k_proj.scale', 'encoder.transformer.layers.23.attention.k_proj.zero_point', 'encoder.transformer.layers.23.attention.k_proj._packed_params.dtype', 'encoder.transformer.layers.23.attention.k_proj._packed_params._packed_params', 'encoder.transformer.layers.23.attention.v_proj.scale', 'encoder.transformer.layers.23.attention.v_proj.zero_point', 'encoder.transformer.layers.23.attention.v_proj._packed_params.dtype', 'encoder.transformer.layers.23.attention.v_proj._packed_params._packed_params', 'encoder.transformer.layers.23.attention.q_proj.scale', 'encoder.transformer.layers.23.attention.q_proj.zero_point', 'encoder.transformer.layers.23.attention.q_proj._packed_params.dtype', 'encoder.transformer.layers.23.attention.q_proj._packed_params._packed_params', 'encoder.transformer.layers.23.attention.out_proj.scale', 'encoder.transformer.layers.23.attention.out_proj.zero_point', 'encoder.transformer.layers.23.attention.out_proj._packed_params.dtype', 'encoder.transformer.layers.23.attention.out_proj._packed_params._packed_params', 'encoder.transformer.layers.23.feed_forward.intermediate_dense.scale', 'encoder.transformer.layers.23.feed_forward.intermediate_dense.zero_point', 'encoder.transformer.layers.23.feed_forward.intermediate_dense._packed_params.dtype', 'encoder.transformer.layers.23.feed_forward.intermediate_dense._packed_params._packed_params', 'encoder.transformer.layers.23.feed_forward.output_dense.scale', 'encoder.transformer.layers.23.feed_forward.output_dense.zero_point', 'encoder.transformer.layers.23.feed_forward.output_dense._packed_params.dtype', 'encoder.transformer.layers.23.feed_forward.output_dense._packed_params._packed_params', 'encoder.readout.scale', 'encoder.readout.zero_point', 'encoder.readout._packed_params.dtype', 'encoder.readout._packed_params._packed_params'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(f=\"wav2vec_small_lm_live2.pt\", map_location=torch.device('cpu')) #device is torch.device('cpu') or torch.device(\"cuda\")\n",
    "model.load_state_dict(state_dict['model_state_dict'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bd751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14a6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "      )\n",
       "      (1): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (2): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (3): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (4): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (5): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "      (6): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (feature_projection): FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): DynamicQuantizedLinear(in_features=1024, out_features=40, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad07f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
