{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f94b11-a2dc-461d-9520-2da01fdba0e3",
   "metadata": {},
   "source": [
    "Create audio samples merged with background noise (saved as wav files), transcribes the samples with the ASR model, and translates the generated transcriptions. Then, saves a csv file containing the path to the original file, noise type, the original transcript, the asr transcript and the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105a7d7-334c-4b00-8053-0b7a35cbccf7",
   "metadata": {},
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6df0a-6129-4c8d-8e56-dea21b7acb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install librosa\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44975bc5-2a2f-4a18-9d28-c5b1241dff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported libraries\n",
    "import librosa\n",
    "import transformers\n",
    "import torchaudio\n",
    "import sentencepiece\n",
    "import numpy as np\n",
    "import math\n",
    "import regex as re\n",
    "import csv\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from transformers import Wav2Vec2ProcessorWithLM, Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b1ad5-9217-4572-a51a-53b1c6e74ad3",
   "metadata": {},
   "source": [
    "The add_background_noise() function uses the librosa library to load both files, adjust the length of the noise file, calculate the RMS to adjust the noise sample and merge both files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9d76fa-8903-4f94-be1c-4f4c1b322c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_background_noise(s_path, n_path, offset=0.0, SNR=10):\n",
    "    '''\n",
    "    Input:\n",
    "    \n",
    "    s_path: a string containing the path for the speech file (.wav format)\n",
    "\n",
    "    n_path: a string containing the path for the noise file (.wav format)\n",
    "\n",
    "    offset: a float representing the starting point, in seconds, to start loading the noise file (default = 0)\n",
    "\n",
    "    SNR: an integer representing the Signal to Noise Ratio (default = 10)\n",
    "    \n",
    "    Returns:\n",
    "    audio sample with noise\n",
    "    '''\n",
    "    sample, sr = librosa.load(s_path)\n",
    "    duration_s = librosa.get_duration(y=sample, sr=sr)\n",
    "    noise, sr = librosa.load(n_path, offset=offset, duration=duration_s) # Load same length of noise file\n",
    "    RMS_s = math.sqrt(np.mean(sample**2))  # Calculate RMS for audio sample\n",
    "    RMS_n = math.sqrt(RMS_s**2/(pow(10,SNR/10)))  # Calculate required RMS for noise\n",
    "    RMS_n_current = math.sqrt(np.mean(noise**2))  # Calculate current RMS for noise\n",
    "    noise = noise*(RMS_n/RMS_n_current)  # Adjust noise\n",
    "    sample_noise = sample + noise \n",
    "    sample_noise = librosa.resample(sample_noise, orig_sr=sr,target_sr=16000)\n",
    "    return sample_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f0156b-2d32-4280-bf9e-ad3da6799d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr_model(wav_file, model, processor):\n",
    "    waveform, sample_rate = torchaudio.load(wav_file)\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000, resampling_method='sinc_interpolation')\n",
    "    # Downsampling is required to match the model's frequency, i.e.\n",
    "    # the frequency of the dataset the model was trained on.\n",
    "    speech_array = resampler(waveform).squeeze().numpy()\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")['input_values']\n",
    "    # Detach the tensors first (otherwise, they require gradients\n",
    "    # to be computed); also, since we're simulating the inference\n",
    "    # squeeze the resulting array at the first dimension (batch_size),\n",
    "    # otherwise, leave it as is, and call batch_decode() instead.\n",
    "    outputs = model(inputs)['logits'].detach().numpy().squeeze(0)\n",
    "    return(processor.decode(outputs).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c9a74b-21fe-45eb-9f7a-91bbe201259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_uk_en(model_name, ukrainian_text):\n",
    "    tokenizer = { model_name : AutoTokenizer.from_pretrained(model_name) }\n",
    "    model = { model_name : AutoModelForSeq2SeqLM.from_pretrained(model_name) }\n",
    "    encoded = { model_name : model[model_name].generate(**tokenizer[model_name](ukrainian_text, return_tensors='pt', padding=True)) }\n",
    "    translation = { model_name : [ tokenizer[model_name].decode([t for t in tensor], skip_special_tokens=True) for tensor in encoded[model_name] ] }\n",
    "    return translation[model_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cbe6da0-133a-4e5a-b372-03e9a29535ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = [\n",
    "    [\"common_voice_uk_32857863.wav\", \"Я чув і знав лиш одне\"],\n",
    "    [\"common_voice_uk_32327955.wav\", \"Бойка дивує цей неймовірний тон але йому зовсім він не до вподоби\"],\n",
    "    [\"common_voice_uk_29194547.wav\", \"Камрад покірно ліг і поклавши голову на лапи зітхнув\"],\n",
    "    [\"common_voice_uk_25651279.wav\", \"Відтоді я встиг об'їхати увесь світ\"],\n",
    "    [\"common_voice_uk_28716623.wav\", \"Червоноармійці вхопили мої руки і скрутили назад\"],\n",
    "    [\"common_voice_uk_26940711.wav\", \"І тут наперед приготуйтеся\"],\n",
    "    [\"common_voice_uk_33459867.wav\", \"Навіщо воно мені здалося\"],\n",
    "    [\"common_voice_uk_32413786.wav\", \"От за це люблю\"],\n",
    "    [\"common_voice_uk_29092801.wav\", \"Любуйся як Дидона стогне\"],\n",
    "    [\"common_voice_uk_25828714.wav\", \"Повертаючись із Орди, помер в дорозі великий Володимирський князь Ярослав Ярославович\"],\n",
    "    [\"common_voice_uk_33339057.wav\", \"Господь вислухав їх молитви\"],\n",
    "    [\"common_voice_uk_21349472.wav\", \"Взяти ноги на плечі\"],\n",
    "    [\"common_voice_uk_27109869.wav\", \"Туди за районного отамана Кузь- менко піде\"],\n",
    "    [\"common_voice_uk_28744536.wav\", \"Що пропаду от лиш не видно\"],\n",
    "    [\"common_voice_uk_21639338.wav\", \"Іде в три дороги\"]\n",
    "]\n",
    "\n",
    "n_list = [\n",
    "    ['noise/bus_ch01.wav', 0.0],\n",
    "    ['noise/cafeteria_ch01.wav', 22.0], \n",
    "    ['noise/car_ch01.wav', 13.0],\n",
    "    ['noise/field_ch01.wav', 79.0],\n",
    "    ['noise/hallway_ch01.wav', 222.0], \n",
    "    ['noise/kitchen_ch01.wav', 145.0],\n",
    "    ['noise/living_ch01.wav', 32.0],\n",
    "    ['noise/meeting_ch01.wav', 3.0],\n",
    "    ['noise/metro_ch01.wav', 0.0],\n",
    "    ['noise/office_ch01.wav', 162.0],\n",
    "    ['noise/park_ch01.wav', 42.0],\n",
    "    ['noise/resto_ch01.wav', 0.0],\n",
    "    ['noise/river_ch01.wav', 85.0], \n",
    "    ['noise/square_ch01.wav', 0.0],\n",
    "    ['noise/station_ch01.wav', 59.0],\n",
    "    ['noise/traffic_ch01.wav', 0.0],\n",
    "    ['noise/washing_ch01.wav', 150.0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1b4428-587a-4251-b021-d2f11304cd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356cbea51c62474a87474b42e610a223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/studio-lab-user/.cache/pyctcdecode/models--Yehor--wav2vec2-xls-r-300m-uk-with-small-lm/snapshots/bbd936400e7566ba44560440aa4abd05b5983c17/language_model/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "sr_model = Wav2Vec2ForCTC.from_pretrained(\"Yehor/wav2vec2-xls-r-300m-uk-with-small-lm\")\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"Yehor/wav2vec2-xls-r-300m-uk-with-small-lm\")\n",
    "\n",
    "nmt_model = \"Helsinki-NLP/opus-mt-tc-big-zle-en\"\n",
    "\n",
    "header = ['path', 'original_transcript', 'noise_type', 'asr_transcript', 'translation']\n",
    "data_translated = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e85ff31-27b9-4922-901d-27a1c1712f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for s_file in s_list:\n",
    "  for n_file in n_list:\n",
    "    # Extract noise type\n",
    "    noise_type = re.findall('/(\\S+?)_ch01\\.wav', n_file[0])[0]\n",
    "    # Save file path\n",
    "    file_path = 'samples_with_noise_snr5/' + re.findall('(\\S+?)\\.wav', s_file[0])[0] + '_' + noise_type + \".wav\"\n",
    "    # Add background noise and save wav file\n",
    "    sr = librosa.get_samplerate(n_file[0])\n",
    "    write(file_path, rate=sr, data=add_background_noise('samples/'+s_file[0], n_file[0], offset=n_file[1], SNR=5))\n",
    "    # Create asr transcript\n",
    "    asr_transcript = asr_model(file_path, sr_model, processor) \n",
    "    # Translate transcript and save all the file info into a list\n",
    "    data_translated.append([file_path, s_file[1], noise_type, asr_transcript, translate_uk_en(nmt_model, asr_transcript)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e4faa-e6a2-435a-96e2-d689c9261b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "with open('cv_translated_with_noise_snr5.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write the data\n",
    "    writer.writerows(data_translated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
